{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Tutorial\n",
    "\n",
    "\n",
    "To get started, you will need your `API_KEY` and `TENANT` in your .env file, check out the readme for more information.  This also assumes you have all of the dependancies installed in requirements.txt, like pandas, requests, and load_dotenv.\n",
    "\n",
    "Data can be extracted from ediphi using the `external/data/pipeline` endpoint.  This is authenticated with an api key, and is a POST call with a payload.  First lets set up a method to call the pipe, grabbing our api key and tenant first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def call_pipe(payload):\n",
    "    url = \"https://api.ediphi.com/api/external/data/pipeline\"\n",
    "    headers = {\n",
    "        \"api-token\": os.getenv(\"API_KEY\"),\n",
    "        \"api-tenant\": os.getenv(\"TENANT\"),\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    res = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "    if res.status_code != 200:\n",
    "        error_msg = (\n",
    "            res.json().get(\"error\", {}).get(\"message\", \"Unknown pipeline error\")\n",
    "        ) + json.dumps(payload)\n",
    "        raise requests.exceptions.HTTPError(error_msg)\n",
    "    return res.json().get(\"data\", {}).get(\"load\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the payload that we pass to this endpoint, here's a method that will return all records of a table by passing the table name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(table_name, properties=None):\n",
    "    if table_name == \"line_items\" and not properties:\n",
    "        raise OverflowError(\"line_items table is too large to load without properties\")\n",
    "\n",
    "    payload = {\n",
    "        f\"#{table_name}#\": {\n",
    "            \"table\": table_name,\n",
    "            \"operation\": {\"method\": \"load.multiple()\"},\n",
    "        }\n",
    "    }\n",
    "    if properties:\n",
    "        payload[f\"#{table_name}#\"][\"operation\"][\"properties\"] = properties\n",
    "\n",
    "    data = call_pipe(payload)\n",
    "    return data.get(table_name, {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first if statement. Please don't ever make a call like `get_table('line_items')` without filtering by properties because this table is typically bigger than all the other tables combined size wise.  This is a big hual of data that could be in the GBs.\n",
    "\n",
    "Generally, other tables like regions can be returned like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = get_table(\"regions\")\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can get resources based on one or more properties like project if we know the project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"UPC\"\n",
    "project = get_table(\"projects\", {\"name\": project_name})\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the payload looks like this, with comments added\n",
    "\n",
    "```\n",
    "{\n",
    "    '#project#':{\n",
    "        'table': 'projects' // check out sample_data.js to see commonly used tables\n",
    "        'operation': {\n",
    "            'method': 'load' // load for one record, load.multiple() for more\n",
    "            'properties':{ // check out the sample_data.js for table properties\n",
    "                'id': '80d8a417-de00-4bdc-bafa-ddd2d753fea5'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The `#project#` can be any string wrapped in `#` and can then used as a property for another resource in the request.  And you can use `load.multiple()` as the method to return more than one record.  \n",
    "\n",
    "For instance, check out this request that returns all estimates for a project based on the project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"UPC\"\n",
    "payload = {\n",
    "    \"#project#\": {\n",
    "        \"table\": \"projects\",\n",
    "        \"operation\": {\n",
    "            \"method\": \"load\",\n",
    "            \"properties\": {\n",
    "                \"name\": project_name\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    \"#estimates#\": {\n",
    "        \"table\": \"estimates\",\n",
    "        \"operation\": {\n",
    "            \"method\": \"load.multiple()\",\n",
    "            \"properties\": {\n",
    "                \"project\": \"#project#\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "data = call_pipe(payload)\n",
    "estimates = data[\"estimates\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from the data/pipeline endpoint look like this:\n",
    "```\n",
    "{\n",
    "    'success': True,\n",
    "    'data':{\n",
    "        'idMap':{},\n",
    "        'variables':{}, \n",
    "        'load':{},\n",
    "        'save':{},\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Within the `data` key, `variables` and `save` are typically `{}`, `idMap` contains all the ids of the returned records, but really the only value you need is `load`, which returns all the records that you requested.  Which is why in the `call_pipe` method, the last line is: `return res.json().get(\"data\", {}).get(\"load\", {})`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas to visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common need is retrieving sort fields and sort codes. Sort fields are like additional columns for line items so that the items can be cross coded. Then there are sort codes in each sort field that can be applied to the line items.  So this payload will get a sort field by name, and all of it's sort codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\"name\": \"Bid Package\"}\n",
    "payload = {\n",
    "    \"#sort_field#\": {\n",
    "        \"table\": \"sort_fields\",\n",
    "        \"operation\": {\n",
    "            \"method\": \"load\",\n",
    "            \"properties\": properties,\n",
    "        },\n",
    "    },\n",
    "    \"#sort_code_{index.count()}#\": {\n",
    "        \"table\": \"sort_codes\",\n",
    "        \"operation\": {\n",
    "            \"method\": \"load.multiple()\",\n",
    "            \"properties\": {\"sort_field\": \"#sort_field#\"},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "data = call_pipe(payload)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is commonly used, so we've added it to the helper functions in here as well.  Another helper function returns all estimate line items, but if you check out the sample data, the sort fields of a line item are within its `extras` property and follow a pattern like `{sort_field.id: sort_code.id}` so the helper function fetches those resources and applies it to the line items is a more useful way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import get_line_items_for_estimate\n",
    "line_items = get_line_items_for_estimate(estimates[0][\"id\"])\n",
    "pd.DataFrame(line_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot more to an ediphi estimate than just line items, such as markups, areas of each floor, rooms, and more. There is a dedicated endpoint to return all of the data of an estimate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_estimate(estimate_id):\n",
    "\n",
    "    url = f\"https://api.ediphi.com/api/external/estimates/{estimate_id}\"\n",
    "    headers = {\n",
    "        \"api-token\": os.getenv(\"API_KEY\"),\n",
    "        \"api-tenant\": os.getenv(\"TENANT\"),\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    res = requests.get(url, headers=headers, timeout=30)\n",
    "    if res.status_code != 200:\n",
    "        error_msg = (\n",
    "            res.json().get(\"error\", {}).get(\"message\", \"Unknown pipeline error\")\n",
    "        )\n",
    "        raise requests.exceptions.HTTPError(error_msg)\n",
    "    return res.json().get(\"sanitizedEstimate\", {})\n",
    "\n",
    "estimate = get_full_estimate(estimates[0][\"id\"])\n",
    "estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may only want to download data that has recently changed, such as estimates that have been updated within the last X days. Currently, we don’t support passing a “date greater than” filter directly in the request payload. However, the key objects that contain significant data—such as Estimates, GCGRs, Bid Tabs, and Schedules—are structured in a way that makes it easy to track changes.\n",
    "\n",
    "For example, with Estimates (more on other objects in separate tutorials, though they follow a similar structure), the database record for an `estimate` contains only its metadata. However, we trigger updates to the `updated_at` timestamp whenever any part of an `estimate` or any of its related child objects changes. Additionally, the parent project’s `updated_at` field is also updated in such cases.\n",
    "\n",
    "A common pattern for retrieving recently updated data is:\n",
    "\n",
    "1.\tFirst, fetch all projects and identify the ones with recent updates.\n",
    "2.\tThen, fetch the recent estimates for each of those projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "\n",
    "def filter_recent_updates(data, days_within):\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_within)\n",
    "    \n",
    "    recent_items = [\n",
    "        item for item in data \n",
    "        if parser.isoparse(item[\"updated_at\"]).replace(tzinfo=None) > cutoff_date \n",
    "        and item.get(\"deleted_at\") is None\n",
    "    ]\n",
    "    \n",
    "    return recent_items\n",
    "\n",
    "projects = get_table(\"projects\")\n",
    "days_within = 7\n",
    "recent_projects = filter_recent_updates(projects, days_within)\n",
    "recent_estimates = []\n",
    "\n",
    "for project in recent_projects:\n",
    "    estimates = get_table(\"estimates\", {\"project\": project[\"id\"]})\n",
    "    recent_estimates.extend(filter_recent_updates(estimates, days_within))\n",
    "\n",
    "recent_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then each full estimate can be requested using the special purpose endpoint used in the `get_full_estimate` method created earlier.  Each one of these is a sizable data haul, so the pattern is to find the ones of interest using their metadata, then only requesting the ones you need.\n",
    "\n",
    "We are planning on improving our api for these use cases to mitigate the need to return all projects. But projects are also a very small as a record, but have many child objects that can be retrieved more directly from this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only resources we support creating with the data pipeline are projects by using `operation.method: save` like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_project(name, region_id, owner_id, industry, task_number):\n",
    "    payload = {\n",
    "        \"#project#\": {\n",
    "            \"table\": \"projects\",\n",
    "            \"operation\": {\n",
    "                \"method\": \"save\",\n",
    "                \"properties\": {\n",
    "                    \"name\": name,\n",
    "                    \"region\": region_id,\n",
    "                    \"owner\": owner_id,\n",
    "                    \"industry\": industry,\n",
    "                    \"task_number\": task_number,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    url = \"https://api.ediphi.com/api/external/data/pipeline\"\n",
    "    \n",
    "    headers = {\n",
    "            \"api-token\": os.getenv(\"API_KEY\"),\n",
    "            \"api-tenant\": os.getenv(\"TENANT\"),\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "    \n",
    "    res = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "\n",
    "    if res.status_code != 200:\n",
    "        raise requests.exceptions.HTTPError(\"pipeline error\")\n",
    "    return res.json().get(\"data\", {}).get(\"save\", {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the minimum required fields are `name, region_id, owner_id, industry, task_number` so lets get some values for those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"test\"\n",
    "regions = get_table(\"regions\")\n",
    "region_id = regions[0][\"id\"]\n",
    "mike = get_table(\"users\", {\"email\": \"michael@ediphi.com\"})[0]\n",
    "owner_id = mike[\"id\"]\n",
    "industries = get_table(\"industries\")\n",
    "industry = industries[0]\n",
    "task_number = \"1.618\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now lets create the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = create_project(name, region_id, owner_id, industry, task_number)\n",
    "new_project = response[\"projects\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be helpful to turn this response into a url you can use to check out your new project, or share the url with the project owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = f\"https://{os.getenv('TENANT')}.ediphi.com/projects/{new_project['id']}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a base understanding of the data pipeline.  If you have any questions reach out to one of our leaders of our data engineering team:\n",
    "\n",
    "Swan Sodja, Senior Data Engineer \n",
    "swan@ediphi.com\n",
    "\n",
    "Colby Ajoku, Director of Partnerships & Integrations\n",
    "colby@ediphi.com\n",
    "\n",
    "Mike Navarro, CTO\n",
    "michael@ediphi.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
