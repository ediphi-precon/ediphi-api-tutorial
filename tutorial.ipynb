{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ediphi Data API Tutorial\n",
    "\n",
    "\n",
    "To get started, you will need your `X_API_KEY` and `DATABASE_NO` in your .env file, check out the readme for more information.  This also assumes you have all of the dependancies installed in requirements.txt, like pandas, requests, and load_dotenv.\n",
    "\n",
    "Data can be extracted from ediphi using endpoints under `data.ediphi.com/api`.  These are authenticated with an api key, and are POST calls, often with url-encoded payloads. \n",
    "\n",
    "At the end, of this document, we'll provide a selection of endpoints for you to explore\n",
    "\n",
    "Let's begin. First let's grab our api key and database number, and set up a method to query the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def query_runner(query: str, df: bool = False):\n",
    "    url = \"https://data.ediphi.com/api/dataset/json\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        \"X-API-KEY\": os.getenv(\"X_API_KEY\"),\n",
    "    }\n",
    "    data = {\n",
    "        \"query\": json.dumps(\n",
    "            {\n",
    "                \"database\": int(os.getenv(\"DATABASE_NO\")),\n",
    "                \"type\": \"native\",\n",
    "                \"native\": {\"query\": f\"{query}\"},\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    if response.status_code != 200:\n",
    "        error_msg = {\n",
    "            \"response\": response.text,\n",
    "            \"data\": data,\n",
    "        }  # this error triggers if there's a problem with your API key\n",
    "        raise requests.exceptions.HTTPError(error_msg)\n",
    "    else:\n",
    "        result = json.loads(response.content)\n",
    "        try:\n",
    "            return result[\n",
    "                \"error\"\n",
    "            ]  # this error triggers if there's a problem with your sql query\n",
    "        except TypeError:\n",
    "            if df:\n",
    "                return pd.DataFrame(result)\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the payload that we pass to this endpoint. Pro tip: it's just a SQL query. So we'll make a basic query and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"select * from regions limit 1\"\n",
    "query_runner(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note: where running queries which will return large result sets, it is necessary to break it into chunks and iterate. This is because the duration of a large query may run past changes made in the primary database. This would couse the attempted query to fail. We provide a helper class in utils.ediphi to demonstrate how this can be done, and we demonstrate its use below. \n",
    "\n",
    "Now, here's a method that will return a data dictionary of your database along with columns and datatypes, as well as each table's relationships to other tables. You can optionally get the relationships for a specific table by passing in its name, and there's also an option to return the result as a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(table_name: str = None, df: bool = False):\n",
    "    with open(\"data_dictionary.sql\", \"r\") as dd:\n",
    "        query = dd.read()\n",
    "    if table_name:\n",
    "        query += f\" where c.table_name = '{table_name}' or cf.references_table = '{table_name}'\"\n",
    "        query += f\" order by c.table_name, cf.references_table\"\n",
    "    return query_runner(query, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run it for the `estimates` table, and we'll set it to return the result as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data_dict(\"estimates\", df=True)\n",
    "display(df.describe())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to join two tables, and we need to know the forign key - primary key relationship. we could use the result of the previous query like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fk_mask = df[\"table_name\"] == \"line_items\"\n",
    "pk_mask = (df[\"table_name\"] == \"estimates\") & df[\"is_pk\"]\n",
    "fk = df.loc[fk_mask, \"column_name\"].values[0]\n",
    "pk = df.loc[pk_mask, \"column_name\"].values[0]\n",
    "print(\"the forign key for line_items conection to estimates is:\", fk)\n",
    "print(\"the primary key it connects to within the estimates table is:\", pk)\n",
    "\n",
    "query = (\n",
    "    \"select e.name as estimate_name, e.phase, l.name as item_name, l.quantity, l.uom, l.total_uc \"\n",
    "    + \"from estimates e \"\n",
    "    + \"join line_items l \"\n",
    "    + f\"on l.{fk} = e.{pk} \"\n",
    "    + \"where e.id = (select id from estimates order by created_at offset 5 limit 1)\"\n",
    ")  # this line is just limiting the result to one estimate\n",
    "\n",
    "df2 = query_runner(query, df=True)\n",
    "display(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common need is retrieving sort fields and sort codes. Sort fields are like additional columns for line items so that the items can be cross coded. Then there are sort codes in each sort field that can be applied to the line items.  So this payload will get a sort field by name, and all of its sort codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = \"Bid Package\"\n",
    "query = f\"select * from sort_codes where sort_field = (select id from sort_fields where name = '{properties}')\"\n",
    "data = query_runner(query)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our helper classes. There are two: `Database`, and `Table`. We'll start by instantiating a `Database` object, and having done that, we'll inspect some of its properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.ediphi as ediphi\n",
    "\n",
    "db = ediphi.Database()\n",
    "print(db.tenant_name)\n",
    "print(\"\\n\\nfirst few tables:\")\n",
    "display(\n",
    "    {k: v for (k, v) in [i for i in db.tables.items()][:5]}\n",
    ")  # first 5 items of the dictionary of all tables in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at a `Table` object and some of its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = ediphi.Table(\"estimates\")\n",
    "print(f\"table id: {estimates.table_id}\\n\\n\")\n",
    "print(\"first few columns:\")\n",
    "display({k: v for (k, v) in [i for i in estimates.columns.items()][:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of this tutorial, we mentioned a demonstration of how to properly iterate though a result set. The example we provide is the `get_table` method of the `Database` class. It uses the `query` method of the `Database` class to fetch chunks of a table ordered by the primary key of the table. Each chunk looks for the highest pk in the last chunk, and fetches results where pk is greater. The default chunk size is 1000 rows, but this can be adjusted. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = db.get_table(\"products\", df=True)\n",
    "display(products.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful methods provided by the `Database` class include `data_dictionary` and `query`. More are coming soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you a base understanding of the data pipeline.  If you have any questions reach out to one of our leaders of our data engineering team:\n",
    "\n",
    "Swan Sodja, Senior Data Engineer \n",
    "swan@ediphi.com\n",
    "\n",
    "Colby Ajoku, Director of Partnerships & Integrations\n",
    "colby@ediphi.com\n",
    "\n",
    "Mike Navarro, CTO\n",
    "michael@ediphi.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
